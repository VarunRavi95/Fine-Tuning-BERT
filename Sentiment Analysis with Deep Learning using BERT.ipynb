{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('smile-annotations-final.csv', \n",
    "                 names = ['id', 'text', 'category'])\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.category.value_counts().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "happy           1137\n",
       "not-relevant     214\n",
       "angry             57\n",
       "surprise          35\n",
       "sad               32\n",
       "disgust            6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_tweet = df[(df['category'] == 'nocode')].index\n",
    "df.drop(index_tweet, inplace=True)\n",
    "df = df[~df.category.str.contains('\\|')]\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'happy': 0,\n",
    "    'not-relevant': 1,\n",
    "    'angry': 2,\n",
    "    'surprise': 3,\n",
    "    'sad': 4,\n",
    "    'disgust': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.category.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.index.values,\n",
    "    df.label.values,\n",
    "    test_size = 0.15,\n",
    "    random_state = 17,\n",
    "    stratify = df.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">angry</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">disgust</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">happy</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">not-relevant</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sad</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">surprise</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text\n",
       "category     label data_type      \n",
       "angry        2     train        48\n",
       "                   val           9\n",
       "disgust      5     train         5\n",
       "                   val           1\n",
       "happy        0     train       966\n",
       "                   val         171\n",
       "not-relevant 1     train       182\n",
       "                   val          32\n",
       "sad          4     train        27\n",
       "                   val           5\n",
       "surprise     3     train        30\n",
       "                   val           5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['category', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Varun\\anaconda3\\envs\\llmenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt')\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt')\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type == 'val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1012,  1030,  2120, 22263,  7301, 26663,  3980,  2006, 12026,\n",
       "          9883,  2363,  2013,  5806,  1012,  2053, 20096,  1012,  1001,  2396,\n",
       "         28556, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  2358,\n",
       "         27421, 14194,  2487, 16425,  2480,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = len(label_dict),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloaders_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=RandomSampler(dataset_train),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "dataloaders_val = DataLoader(\n",
    "    dataset=dataset_val,\n",
    "    sampler=RandomSampler(dataset_val),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Varun\\anaconda3\\envs\\llmenv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(dataloaders_train)*epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    preds_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds == label])}/{len(y_true)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing Layers for Fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight True\n",
      "bert.encoder.layer.4.attention.self.query.bias True\n",
      "bert.encoder.layer.4.attention.self.key.weight True\n",
      "bert.encoder.layer.4.attention.self.key.bias True\n",
      "bert.encoder.layer.4.attention.self.value.weight True\n",
      "bert.encoder.layer.4.attention.self.value.bias True\n",
      "bert.encoder.layer.4.attention.output.dense.weight True\n",
      "bert.encoder.layer.4.attention.output.dense.bias True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.intermediate.dense.weight True\n",
      "bert.encoder.layer.4.intermediate.dense.bias True\n",
      "bert.encoder.layer.4.output.dense.weight True\n",
      "bert.encoder.layer.4.output.dense.bias True\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.attention.self.query.weight True\n",
      "bert.encoder.layer.5.attention.self.query.bias True\n",
      "bert.encoder.layer.5.attention.self.key.weight True\n",
      "bert.encoder.layer.5.attention.self.key.bias True\n",
      "bert.encoder.layer.5.attention.self.value.weight True\n",
      "bert.encoder.layer.5.attention.self.value.bias True\n",
      "bert.encoder.layer.5.attention.output.dense.weight True\n",
      "bert.encoder.layer.5.attention.output.dense.bias True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.intermediate.dense.weight True\n",
      "bert.encoder.layer.5.intermediate.dense.bias True\n",
      "bert.encoder.layer.5.output.dense.weight True\n",
      "bert.encoder.layer.5.output.dense.bias True\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.attention.self.query.weight True\n",
      "bert.encoder.layer.6.attention.self.query.bias True\n",
      "bert.encoder.layer.6.attention.self.key.weight True\n",
      "bert.encoder.layer.6.attention.self.key.bias True\n",
      "bert.encoder.layer.6.attention.self.value.weight True\n",
      "bert.encoder.layer.6.attention.self.value.bias True\n",
      "bert.encoder.layer.6.attention.output.dense.weight True\n",
      "bert.encoder.layer.6.attention.output.dense.bias True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.intermediate.dense.weight True\n",
      "bert.encoder.layer.6.intermediate.dense.bias True\n",
      "bert.encoder.layer.6.output.dense.weight True\n",
      "bert.encoder.layer.6.output.dense.bias True\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.attention.self.query.weight True\n",
      "bert.encoder.layer.7.attention.self.query.bias True\n",
      "bert.encoder.layer.7.attention.self.key.weight True\n",
      "bert.encoder.layer.7.attention.self.key.bias True\n",
      "bert.encoder.layer.7.attention.self.value.weight True\n",
      "bert.encoder.layer.7.attention.self.value.bias True\n",
      "bert.encoder.layer.7.attention.output.dense.weight True\n",
      "bert.encoder.layer.7.attention.output.dense.bias True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.intermediate.dense.weight True\n",
      "bert.encoder.layer.7.intermediate.dense.bias True\n",
      "bert.encoder.layer.7.output.dense.weight True\n",
      "bert.encoder.layer.7.output.dense.bias True\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.attention.self.query.weight True\n",
      "bert.encoder.layer.8.attention.self.query.bias True\n",
      "bert.encoder.layer.8.attention.self.key.weight True\n",
      "bert.encoder.layer.8.attention.self.key.bias True\n",
      "bert.encoder.layer.8.attention.self.value.weight True\n",
      "bert.encoder.layer.8.attention.self.value.bias True\n",
      "bert.encoder.layer.8.attention.output.dense.weight True\n",
      "bert.encoder.layer.8.attention.output.dense.bias True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.intermediate.dense.weight True\n",
      "bert.encoder.layer.8.intermediate.dense.bias True\n",
      "bert.encoder.layer.8.output.dense.weight True\n",
      "bert.encoder.layer.8.output.dense.bias True\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.attention.self.query.weight True\n",
      "bert.encoder.layer.9.attention.self.query.bias True\n",
      "bert.encoder.layer.9.attention.self.key.weight True\n",
      "bert.encoder.layer.9.attention.self.key.bias True\n",
      "bert.encoder.layer.9.attention.self.value.weight True\n",
      "bert.encoder.layer.9.attention.self.value.bias True\n",
      "bert.encoder.layer.9.attention.output.dense.weight True\n",
      "bert.encoder.layer.9.attention.output.dense.bias True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.intermediate.dense.weight True\n",
      "bert.encoder.layer.9.intermediate.dense.bias True\n",
      "bert.encoder.layer.9.output.dense.weight True\n",
      "bert.encoder.layer.9.output.dense.bias True\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.attention.self.query.weight True\n",
      "bert.encoder.layer.10.attention.self.query.bias True\n",
      "bert.encoder.layer.10.attention.self.key.weight True\n",
      "bert.encoder.layer.10.attention.self.key.bias True\n",
      "bert.encoder.layer.10.attention.self.value.weight True\n",
      "bert.encoder.layer.10.attention.self.value.bias True\n",
      "bert.encoder.layer.10.attention.output.dense.weight True\n",
      "bert.encoder.layer.10.attention.output.dense.bias True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.intermediate.dense.weight True\n",
      "bert.encoder.layer.10.intermediate.dense.bias True\n",
      "bert.encoder.layer.10.output.dense.weight True\n",
      "bert.encoder.layer.10.output.dense.bias True\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('bert.encoder.layer.0'):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('bert.encoder.layer.1'):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('bert.encoder.layer.2'):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('bert.encoder.layer.3'):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('bert.encoder.layer.4'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.5'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.6'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.7'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.8'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.9'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.10'):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith('bert.encoder.layer.11'):\n",
    "        param.requires_grad = True\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58a188aa0c641848ce2b6e6507d6265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b2f5aab6794451b8e17e2a81ff5439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.7688178619340299\n",
      "Validation Loss: 0.5637298737253461\n",
      "F1 Score(weighted): 0.7799931010693343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285a058704fd40fcbf0bd1f6ec0fd672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.4567030928659415\n",
      "Validation Loss: 0.5674779628004346\n",
      "F1 Score(weighted): 0.8175910700228352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9d551cc1fa45f6a79e173bf34ccf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.3270618980099994\n",
      "Validation Loss: 0.5491878688335419\n",
      "F1 Score(weighted): 0.8502763330763634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2ef443ccbb442ca48aa9984581d0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.21910040492214086\n",
      "Validation Loss: 0.6456359901598522\n",
      "F1 Score(weighted): 0.8637598262720534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c7760e99f44242968f42794744d9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.15404614857508844\n",
      "Validation Loss: 0.6057738895927157\n",
      "F1 Score(weighted): 0.8723406490146413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e297c36b0b744a9f8202a156de56fd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.09958732379692799\n",
      "Validation Loss: 0.6959014875548226\n",
      "F1 Score(weighted): 0.8557832916378623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c01088d0b341da8bf33bb89ed80cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.07024346318683543\n",
      "Validation Loss: 0.7033327498606273\n",
      "F1 Score(weighted): 0.8647895245428878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aae652fcd6f4f4abbed4c6c192c48b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.046010625116260986\n",
      "Validation Loss: 0.6727561141763415\n",
      "F1 Score(weighted): 0.8704639672476381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f1ec0fa184d4081cea5e34dce31ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.03320169370623291\n",
      "Validation Loss: 0.659348155770983\n",
      "F1 Score(weighted): 0.8773808910193632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059cf9fe481e4f95809e0a50089e7417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch{epoch}\n",
      "Training Loss: 0.02904746021829649\n",
      "Validation Loss: 0.6715543035949979\n",
      "F1 Score(weighted): 0.8692851447546528\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloaders_train, desc = 'Epoch {:1d}'.format(epoch),\n",
    "                       leave = False,\n",
    "                       disable = False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    torch.save(model.state_dict(), f'Models/BERT_ft_epoch{epoch}.model')\n",
    "    \n",
    "    tqdm.write('\\nEpoch{epoch}')\n",
    "    loss_train_avg = loss_train_total/len(dataloaders_train)\n",
    "    tqdm.write(f'Training Loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloaders_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation Loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score(weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Models/BERT_ft_epoch10.model',\n",
    "                                map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,predictions, true_vals = evaluate(dataloaders_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: happy\n",
      "Accuracy: 166/171\n",
      "Class: not-relevant\n",
      "Accuracy: 18/32\n",
      "Class: angry\n",
      "Accuracy: 8/9\n",
      "Class: surprise\n",
      "Accuracy: 3/5\n",
      "Class: sad\n",
      "Accuracy: 1/5\n",
      "Class: disgust\n",
      "Accuracy: 0/1\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
